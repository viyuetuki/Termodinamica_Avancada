{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treinando uma rede neural\n",
    "=========================\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdução\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depois de uma longa jornada, finalmente chegamos ao *season finale* da nossa saga para construir uma rede neural artificial em Python puro. Agora que já conseguimos criar uma rede neural, o próximo passo é treinar essa rede.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivo\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treinar uma rede neural artificial tipo Multilayer Perceptron usando Python.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importações\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from classes import Valor\n",
    "from funcoes import plota_grafo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Código e discussão\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo estão as classes que criamos no notebook anterior. Elas são as classes de base para criarmos nossa rede neural MLP. Ao longo deste notebook vamos fazer as últimas modificações nelas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuronio:\n",
    "    def __init__(self, num_dados_entrada):\n",
    "        pesos = []\n",
    "\n",
    "        for _ in range(num_dados_entrada):\n",
    "            peso = Valor(random.uniform(-1, 1))\n",
    "            pesos.append(peso)\n",
    "\n",
    "        self.pesos = pesos\n",
    "        self.vies = Valor(random.uniform(-1, 1))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"Executa a computação interna do neurônio.\n",
    "\n",
    "        Args:\n",
    "          x: lista de valores de entrada (informação que chega ao neurônio).\n",
    "\n",
    "        Returns:\n",
    "          Informação que o neurônio propaga adiante. Trata-se da\n",
    "          aplicação da função de ativação à soma de `x` vezes os pesos do\n",
    "          neurônio adicionado ao viés.\n",
    "        \"\"\"\n",
    "        assert len(x) == len(self.pesos), \"Seu x tem tamanho errado.\"\n",
    "\n",
    "        soma = 0\n",
    "        for x_, p in zip(x, self.pesos):\n",
    "            soma = soma + x_ * p\n",
    "\n",
    "        soma = soma + self.vies\n",
    "        dado_de_saida = soma.sig()\n",
    "        return dado_de_saida\n",
    "\n",
    "\n",
    "class Camada:\n",
    "    def __init__(self, num_dados_entrada, num_neuronios):\n",
    "        neuronios = []\n",
    "\n",
    "        for _ in range(num_neuronios):\n",
    "            neuronio = Neuronio(num_dados_entrada)\n",
    "            neuronios.append(neuronio)\n",
    "\n",
    "        self.neuronios = neuronios\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"Executa a computação de cada neurônio da camada.\n",
    "\n",
    "        Args:\n",
    "          x: lista de valores de entrada (informação que chega aos neurônios).\n",
    "\n",
    "        Returns:\n",
    "          Informação que os neurônios da camada propagam adiante.\n",
    "        \"\"\"\n",
    "        saidas = []\n",
    "\n",
    "        for neuronio in self.neuronios:\n",
    "            informacao = neuronio(x)\n",
    "            saidas.append(informacao)\n",
    "\n",
    "        if len(saidas) == 1:\n",
    "            return saidas[0]\n",
    "        else:\n",
    "            return saidas\n",
    "\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, num_dados_entrada, num_neuronios_por_camada):\n",
    "\n",
    "        percurso = [num_dados_entrada] + num_neuronios_por_camada\n",
    "        camadas = []\n",
    "\n",
    "        for i in range(len(num_neuronios_por_camada)):\n",
    "            camada = Camada(percurso[i], percurso[i+1])\n",
    "            camadas.append(camada)\n",
    "\n",
    "        self.camadas = camadas\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"Executa a computação de cada camada.\n",
    "\n",
    "        Como a rede MLP é uma rede feedforward, a informação que uma camada\n",
    "        recebe é a informação de saída da camada anterior. A única exceção é a\n",
    "        camada de entrada, onde a informação é fornecida pelo usuário\n",
    "\n",
    "        Args:\n",
    "          x: informação fornecida na camada de entrada.\n",
    "\n",
    "        Returns:\n",
    "          Informação recuperada na camada de saída.\n",
    "        \"\"\"\n",
    "        for camada in self.camadas:\n",
    "            x = camada(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A função de perda (*loss function*)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fomos ao laboratório e sintetizamos 4 amostras de [complete com o que quiser]. Cada uma dessas amostras foi feita variando 3 parâmetros de processamento diferentes. Estas amostras e seus parâmetros de processamento estão representados na variável `x` abaixo.\n",
    "\n",
    "Levamos essas 4 amostras no equipamento [complete com o que quiser] e obtivemos uma resposta que está representada na variável `y_true` abaixo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [\n",
    "  [2.0, 3.0, -1.0],\n",
    "  [3.0, -1.0, 0.5],\n",
    "  [0.5, 1.0, 1.0],\n",
    "  [1.0, 1.0, -1.0],\n",
    "]\n",
    "\n",
    "y_true = [1, 0, 0.2, 0.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Queremos treinar uma MLP que seja capaz de modelar o comportamento que observamos. Isto é, queremos uma rede neural que receba informação sobre os 3 parâmetros de processamento e que com isso seja capaz de prever qual o resultado que teríamos no equipamento que mencionamos no parágrafo anterior.\n",
    "\n",
    "Vamos, primeiramente, criar uma rede neural simples do tipo MLP.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_DADOS_DE_ENTRADA = 3  # são 3 parâmetros que descrevem cada amostra\n",
    "NUM_DADOS_DE_SAIDA = 1    # queremos apenas um valor de saída por amostra\n",
    "CAMADAS_OCULTAS = [3, 2]  # fique à vontade para alterar aqui\n",
    "\n",
    "arquitetura_da_rede = CAMADAS_OCULTAS + [NUM_DADOS_DE_SAIDA]\n",
    "\n",
    "minha_mlp = MLP(NUM_DADOS_DE_ENTRADA, arquitetura_da_rede)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com nossa rede criada, podemos realizar uma previsão! Mas muito provavelmente esta previsão será bastante subótima.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "\n",
    "for x_ in x:\n",
    "    valor_previsto = minha_mlp(x_)\n",
    "    y_pred.append(valor_previsto)\n",
    "\n",
    "print(y_true)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nosso resultado, assim como esperado, é bastante subótimo. Mas como quantificar isso? Seria bom se existisse uma forma de quantificar em um único número o quão boa está a previsão da minha rede.\n",
    "\n",
    "Uma forma de quantificar a qualidade da previsão é usando uma `função de perda`, mais conhecida como `loss function`. Aqui podemos, por exemplo, computar a soma dos erros quadráticos. Esta é uma função de perda muito usada em problemas de regressão.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erros_quadrados = []\n",
    "\n",
    "for yt, yp in zip(y_true, y_pred):\n",
    "    erro_quadratico = (yp - yt) ** 2\n",
    "    erros_quadrados.append(erro_quadratico)\n",
    "\n",
    "loss = sum(erros_quadrados)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A variável `loss` é uma medida da performance da rede neural que criamos. Essa variável é uma instância de `Valor`, logo podemos facilmente observar o grafo computacional desta métrica.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.propagar_tudo()\n",
    "grafo = plota_grafo(loss)\n",
    "grafo.render(\"rede_neural\", format=\"png\")\n",
    "grafo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinando a rede através da atualização dos parâmetros\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vimos acima, o grafo computacional da nossa rede neural é bastante complexo! Cheio de vértices!\n",
    "\n",
    "Nosso objetivo é treinar a rede neural que criamos, sendo que para isso precisamos alterar os parâmetros internos da rede. O primeiro passo para fazer isso é alterar as nossas classes de forma que o código abaixo funcione.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parametros = minha_mlp.parametros()\n",
    "\n",
    "print(len(parametros))\n",
    "print()\n",
    "parametros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que conseguimos &ldquo;extrair&rdquo; os parâmetros da nossa MLP, podemos fazer o treino deles! Temos que pensar bem como alterar os parâmetros da maneira correta. Lembre-se que nosso desejo é reduzir a métrica computada pela função de perda.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parametros:\n",
    "    ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que executamos um &ldquo;ciclo&rdquo; de treino, podemos observar o impacto disso!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "for x_ in x:\n",
    "    valor_previsto = minha_mlp(x_)\n",
    "    y_pred.append(valor_previsto)\n",
    "\n",
    "erros_quadrados = []\n",
    "for yt, yp in zip(y_true, y_pred):\n",
    "    erro_quadratico = (yp - yt) ** 2\n",
    "    erros_quadrados.append(erro_quadratico)\n",
    "\n",
    "loss = sum(erros_quadrados)\n",
    "loss.propagar_tudo()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A descida do gradiente\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apenas um &ldquo;ciclo&rdquo; de treino costuma não ser suficiente para treinarmos uma rede neural, precisamos de mais! Antes de seguir em frente, vamos definir um termo: chamamos de `época` toda vez que nossa rede neural propaga *todo* nosso dataset. É comum treinarmos redes neurais por dezenas, centenas e até milhares de épocas!\n",
    "\n",
    "Vamos programar um treino completo da rede neural!\n",
    "\n",
    "Um detalhe: sempre temos que zerar os gradientes antes de fazer o backpropagation. Isso é necessário pois cada vez que alteramos os parâmetros estaremos em outra posição da curva de perda e os gradientes antigos já não são mais válidos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_DADOS_DE_ENTRADA = 3  # são 3 parâmetros que descrevem cada amostra\n",
    "NUM_DADOS_DE_SAIDA = 1    # queremos apenas um valor de saída por amostra\n",
    "CAMADAS_OCULTAS = [3, 2]  # fique à vontade para alterar aqui\n",
    "\n",
    "x = [\n",
    "    [2.0, 3.0, -1.0],\n",
    "    [3.0, -1.0, 0.5],\n",
    "    [0.5, 1.0, 1.0],\n",
    "    [1.0, 1.0, -1.0],\n",
    "]\n",
    "\n",
    "y_true = [1, 0, 0.2, 0.5]\n",
    "\n",
    "arquitetura_da_rede = CAMADAS_OCULTAS + [NUM_DADOS_DE_SAIDA]\n",
    "minha_mlp = MLP(NUM_DADOS_DE_ENTRADA, arquitetura_da_rede)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCAS = 10\n",
    "\n",
    "for epoca in range(NUM_EPOCAS):\n",
    "    # forward pass\n",
    "    ???\n",
    "\n",
    "    # zero grad\n",
    "    ???\n",
    "    \n",
    "    # loss\n",
    "    ???    \n",
    "\n",
    "    # backpropagation\n",
    "    ???\n",
    "\n",
    "    # atualiza parâmetros\n",
    "    ???\n",
    "\n",
    "    # mostra resultado\n",
    "    ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após o treino, podemos checar se nossa rede é capaz de prever os dados que coletamos com boa performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_true)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esse processo de atualizar os parâmetros da rede neural observando os gradientes locais é chamado de `descida do gradiente` (ou apenas `método do gradiente`).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusão\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ilumpy",
   "language": "python",
   "name": "ilumpy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "org": null
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
